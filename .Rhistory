# Reading the billboard.csv file
billboard=read.csv("https://raw.githubusercontent.com/dpuelz/STA380/master/data/billboard.csv")
library(dplyr)
library(tidyverse)
library(kableExtra)
most_popular_songs=billboard %>%
group_by(performer,song) %>%
summarize(count=n(), .groups = 'drop') %>%
arrange(desc(count)) %>%
head(10)
knitr::kable(most_popular_songs, col.names = c("Performer","Song","Count"))
billboard %>%
filter(year!=1958 & year!=2021) %>% # Filtering out the years 1958 and 2021
group_by(year) %>%
summarize(musical_diversity=length(unique(song)))%>%
ggplot(aes(x = year, y = musical_diversity))+
geom_line()
labs(x = "Year", y = "Musical Diversity",caption="The musical diversity peaked in 1970 with about 800 unique songs that appeared in the Billboard Top 100. The musical diversity then decreased steadily, almost halfing in the beginning of the 21st century. The musical diversity has been continuously increasing since then reaching 1970 levels in 2020")
# Filtering out the 10-week hit songs
ten_week_hit = billboard %>%
group_by(performer, song) %>%
summarize(week_hits = n(), .groups='drop') %>%
filter(week_hits >= 10)
# Filtering out artists who had at least 30 songs that were 10 week hits
dat = artist_at_least_thirty_ten_week_hits=ten_week_hit %>%
group_by(performer) %>%
summarize(count_10_week_hits= n()) %>%
filter(count_10_week_hits >= 30)
#print(dim(artist_at_least_30_ten_week_hits))
plt1 = ggplot(dat, aes(x = performer, y = count_10_week_hits))+
geom_col()+
coord_flip()+
labs(x = "Performer", y = "No of 10 week hits")
#artist_at_least_thirty_ten_week_hits
plt1
library(dplyr)
library(mosaic)
library(tidyverse)
raw_buildings_data <-read.csv('https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv')
raw_buildings_data$renovated <- as.factor(raw_buildings_data$renovated )
raw_buildings_data$class_a <- as.factor(raw_buildings_data$class_a )
raw_buildings_data$class_b <- as.factor(raw_buildings_data$class_b )
raw_buildings_data$LEED <- as.factor(raw_buildings_data$LEED )
raw_buildings_data$Energystar <- as.factor(raw_buildings_data$Energystar )
raw_buildings_data$green_rating <- as.factor(raw_buildings_data$green_rating )
raw_buildings_data$amenities <- as.factor(raw_buildings_data$amenities )
raw_buildings_data$net <- as.factor(raw_buildings_data$net )
raw_buildings_data$class <- ifelse(raw_buildings_data$class_a == 1,'A',ifelse(raw_buildings_data$class_b == 1,'B','C'))
raw_buildings_data %>% filter(leasing_rate >10)%>%group_by(green_rating)%>%summarise(med_rent = median(Rent),  mean_rent = mean(Rent), count = n())
ggplot(raw_buildings_data, aes(green_rating, Rent)) + geom_boxplot()
ggplot(raw_buildings_data, aes(age,leasing_rate, color= green_rating)) + geom_point()
ggplot(raw_buildings_data, aes(leasing_rate, Rent, color = green_rating)) + geom_point()
raw_buildings_data %>% group_by(green_rating)%>%summarise(median_rate = median(leasing_rate), count = n())
raw_buildings_data%>%group_by(green_rating)%>%summarise(median_size = median(size), count = n())
ggplot(raw_buildings_data, aes(size,Rent,color = green_rating)) + geom_point()
raw_buildings_data%>%group_by(green_rating, amenities)%>%summarise(median(Rent),count=n())
raw_buildings_data%>%group_by(green_rating)%>%summarise(median(Rent),median(age),count=n())
ggplot(raw_buildings_data, aes(age,Rent,color = green_rating)) + geom_point()
raw_buildings_data%>%group_by(green_rating)%>%summarise(median(stories),median(Rent), count= n())
ggplot(raw_buildings_data, aes(stories,Rent, color = green_rating)) + geom_point()
raw_buildings_data%>%filter(cluster>=430 & cluster<=600)%>%group_by(green_rating)%>%summarise(median(Rent), count= n())
ggplot(raw_buildings_data, aes(cluster, Rent, color = green_rating)) + geom_point()
#Reading the CapMetro csv file
capmetro=read.csv("https://raw.githubusercontent.com/dpuelz/STA380/master/data/capmetro_UT.csv")
#print(capmetro)
# Extracting the data for average boarding happening every hour
avg_boarding_every_hour = capmetro %>%
group_by(hour_of_day, day_of_week) %>%
summarise(avgboarding = mean(boarding), .groups='drop') %>%
arrange(day_of_week)
#avg_boarding_every_hour
ggplot(avg_boarding_every_hour) +
geom_line(aes(y=avgboarding, x= hour_of_day))+
facet_wrap(~day_of_week)+
labs(title = "Capital Metro Boarding",y = "Average Boarding",x = "Time",caption="The average boarding on weekdays is much more than that on the weekends(Saturday/Sunday). The boarding steadily increases after 6:00 peaking at around 15:00-16:00 on all weekdays")
#Extracting the data for average alighting every hour
avg_alighting_every_hour = capmetro %>%
group_by(hour_of_day, day_of_week) %>%
summarise(avg_alighting = mean(alighting), .groups='drop') %>%
arrange(day_of_week)
#avg_alighting_every_hour
ggplot(avg_alighting_every_hour) +
geom_line(aes(y=avg_alighting , x= hour_of_day))+
facet_wrap(~day_of_week)+
labs(title = "Capital Metro Alighting",y = "Average Alighting",x = "Time",caption="The average alighting on weekdays is much more than that on the weekends(Saturday/Sunday). The alighting steadily increases after 6:00 peaking at around 8:00-9:00 on all weekdays, probably due to the fact that working people might be getting off the bus after taking a ride to office")
ggplot(data=capmetro,aes(x=temperature,y=boarding))+
geom_point() +
labs(title = "Boarding vs Temperature",y = "Boarding",x = "Temperature",caption="From the scatter plot above, the boarding looks independent of the temperature")
ggplot(data=capmetro,aes(x=temperature,y=alighting))+
geom_point() +
labs(title = "Alighting vs Temperature",y = "Alighting",x = "Temperature",caption="From the scatter plot above, the alighting also looks independent of the temperature")
library(mosaic)
library(quantmod)
library(foreach)
# Import sets of  ETFs
safe_pf = c("VOO", "SCHA", "VXUS", "VWO", "AOR")
high_pf = c("TQQQ","QLD","SPXL","SPUU","SWAN","NTSX")
med_pf = c("VOO","SCHA","VXUS","SPUU","SWAN","NTSX")
safe_pf.data = getSymbols(safe_pf, from = "2014-01-01")
high_pf.data = getSymbols(high_pf, from = "2014-01-01")
med_pf.data = getSymbols(med_pf, from = "2014-01-01")
#adjust for  splits and dividents
VOOa <- adjustOHLC(VOO)
SCHAa <- adjustOHLC(SCHA)
VXUSa <- adjustOHLC(VXUS)
VWOa <- adjustOHLC(VWO)
AORa <- adjustOHLC(AOR)
TQQQb <- adjustOHLC(TQQQ)
QLDb <- adjustOHLC(QLD)
SPXLb <- adjustOHLC(SPXL)
SPUUb <- adjustOHLC(SPUU)
SWANb <- adjustOHLC(SWAN)
NTSXb <- adjustOHLC(NTSX)
VOOc <- adjustOHLC(VOO)
SCHAc <- adjustOHLC(SCHA)
VXUSc <- adjustOHLC(VXUS)
SPUUc <- adjustOHLC(SPUU)
SWANc <- adjustOHLC(SWAN)
NTSXc <- adjustOHLC(NTSX)
all_returns_safe <- cbind(ClCl(VOOa),ClCl(SCHAa), ClCl(VXUSa),ClCl(VWOa),ClCl(AORa))
all_returns_high <- cbind(ClCl(TQQQb),ClCl(QLDb), ClCl(SPXLb),ClCl(SPUUb),ClCl(SWANb),ClCl(NTSXb))
all_returns_med <- cbind(ClCl(VOOc),ClCl(SCHAc), ClCl(VXUSc),ClCl(SPUUc),ClCl(SWANc),ClCl(NTSXc))
all_returns_safe = as.matrix(na.omit(all_returns_safe))
all_returns_high = as.matrix(na.omit(all_returns_high))
all_returns_med = as.matrix(na.omit(all_returns_med))
head(all_returns_safe)
head(all_returns_high)
head(all_returns_med)
pairs(all_returns_safe)
pairs(all_returns_high)
pairs(all_returns_med)
return.today_safe <- resample(all_returns_safe, 1, orig.ids = FALSE)
return.today_high <- resample(all_returns_high, 1, orig.ids = FALSE)
return.today_med <- resample(all_returns_med, 1, orig.ids = FALSE)
total_wealth = 100000
my_weights_safe <- c(0.2, 0.2, 0.2, 0.2, 0.2)
my_weights_high <- c(0.16, 0.16, 0.16, 0.16,0.16)
my_weights_med <- c(0.20, 0.14, 0.20, 0.14, 0.14, 0.32)
holdings_safe <- total_wealth*my_weights_safe
holdings_high <- total_wealth*my_weights_high
holdings_med <- total_wealth*my_weights_med
holdings_safe <- holdings_safe*(1+return.today_safe)
holdings_high <- holdings_high*(1+return.today_high)
holdings_med <- holdings_med*(1+return.today_med)
total_wealth_safe <- sum(holdings_safe)
total_wealth_high <- sum(holdings_high)
total_weatlh_med <- sum(holdings_med)
total_wealth_safe
total_wealth_high
total_weatlh_med
initial_wealth = 100000
sim1 = foreach(i = 1:5000, .combine = "rbind")%do%{
total_wealth_safe = initial_wealth
my_weights_safe <- c(0.2, 0.2, 0.2, 0.2, 0.2)
holdings_safe <- my_weights_safe*total_wealth_safe
n_days = 20
wealthtracker_safe = rep(0, n_days)
for(today in 1:n_days){
return.today_safe <- resample(all_returns_safe, 1, orig.ids = FALSE)
holdings_safe <- holdings_safe*(1+return.today_safe)
total_wealth_safe <- sum(holdings_safe)
wealthtracker_safe[today] = total_wealth_safe
}
wealthtracker_safe
}
head(sim1)
hist(sim1[,n_days], labels = TRUE,25)
VAR = quantile(sim1[,n_days] - initial_wealth, .05)
VAR
initial_wealth = 100000
sim2 = foreach(i = 1:5000, .combine = "rbind")%do%{
total_wealth_high = initial_wealth
my_weights_high <- c(0.16, 0.16, 0.16, 0.16,0.16)
holdings_high <- my_weights_high*total_wealth_high
n_days = 20
wealthtracker_high = rep(0, n_days)
for(today in 1:n_days){
return.today_high <- resample(all_returns_high, 1, orig.ids = FALSE)
holdings_high <- holdings_high*(1+return.today_high)
total_wealth_high <- sum(holdings_high)
wealthtracker_high[today] = total_wealth_high
}
wealthtracker_high
}
head(sim2)
hist(sim2[,n_days], labels = TRUE,25)
VAR = quantile(sim2[,n_days] - initial_wealth, .05)
VAR
initial_wealth = 100000
sim3 = foreach(i = 1:5000, .combine = "rbind")%do%{
total_wealth_med = initial_wealth
my_weights_med <- c(0.20, 0.14, 0.20, 0.14, 0.14, 0.32)
holdings_med <- my_weights_med*total_wealth_med
n_days = 20
wealthtracker_med = rep(0, n_days)
for(today in 1:n_days){
return.today_med <- resample(all_returns_med, 1, orig.ids = FALSE)
holdings_med <- holdings_med*(1+return.today_med)
total_wealth_med <- sum(holdings_med)
wealthtracker_med[today] = total_wealth_med
}
wealthtracker_med
}
head(sim3)
hist(sim3[,n_days], labels = TRUE,25)
VAR = quantile(sim3[,n_days] - initial_wealth, .05)
VAR
# Reading the wine data
wine=read.csv("https://raw.githubusercontent.com/dpuelz/STA380/master/data/wine.csv")
#print(wine)
# Scaling the wine data
wine_scale=wine %>%
select (-quality, -color)%>%
scale(center=TRUE, scale=TRUE)
# Running the PCA on the data
wine_PCA=prcomp(wine_scale)
summary(wine_PCA)
wine_loading_score = data.frame(wine_PCA$rotation)
#print(wine_loading_score)
wine_x_score=data.frame(wine_PCA$x)
print(wine_x_score)
qplot(wine_x_score[,1], wine_x_score[,2], color=factor(wine$color), xlab='PC 1', ylab='PC 2')
qplot(wine_x_score[,1], wine_x_score[,2], color=factor(wine$quality), xlab='PC 1', ylab='PC 2')
library(ggalt)
cluster_kmeans = kmeans(wine_scale, 2, nstart=25)
cluster_kmeans$centers
cluster_kmeans$cluster
qplot(pH, color, data=wine,color=factor(cluster_kmeans$cluster))
qplot(alcohol,color,data=wine,color=factor(cluster_kmeans$cluster))
qplot(sulphates,color,data=wine,color=factor(cluster_kmeans$cluster))
cluster_kmeans = kmeans(wine_scale, 7, nstart=25)
qplot(pH, quality, data=wine,color=factor(cluster_kmeans$cluster))
qplot(alcohol,quality,data=wine,color=factor(cluster_kmeans$cluster))
qplot(sulphates,quality,data=wine,color=factor(cluster_kmeans$cluster))
social_marketing=read.csv("https://raw.githubusercontent.com/dpuelz/STA380/master/data/social_marketing.csv")
# Data Cleaning: Removing the spam, chatter, adult and uncategorized variables as they would not lead to useful insights. Also removed the X variable
social_marketing=subset(social_marketing,select= -c(chatter,spam,adult,uncategorized,X))
social_marketing=na.omit(social_marketing)
# Scaling the data
social_marketing_scale=scale(social_marketing,center=TRUE, scale=TRUE)
#summary(social_marketing_scale)
#Find the optimal K
kmean_withinss <- function(k) {
cluster <- kmeans(social_marketing_scale,k)
return (cluster$tot.withinss)
}
# Set maximum cluster
max_k <-20
# Run algorithm over a range of k
wss <- sapply(2:max_k, kmean_withinss)
# Create a data frame to plot the graph
elbow <-data.frame(2:max_k, wss)
# Plot the graph with gglop
ggplot(elbow, aes(x = X2.max_k, y = wss)) +
geom_point() +
geom_line() +
scale_x_continuous(breaks = seq(1, 20, by = 1))
cluster_kmeans=kmeans(social_marketing_scale,4,nstart=25)
#qplot(outdoors,fashion,data=social_marketing,color=factor(cluster_all$cluster))
print(names(sort(cluster_kmeans$centers[1,])[28:32]))
#plot1 = social_marketing_scale[,names(sort(cluster_kmeans$centers[1,])[28:32])]
#pairs(plot1,col=factor(cluster_kmeans$cluster))
print(names(sort(cluster_kmeans$centers[2,])[28:32]))
print(names(sort(cluster_kmeans$centers[3,])[28:32]))
print(names(sort(cluster_kmeans$centers[4,])[28:32]))
library(tm)
library(tidyverse)
library(slam)
library(proxy)
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
# Get list of all authors
#path_list = Sys.glob('../ReutersC50/*/*/*')
path_list = Sys.glob('C:/Users/smjoh/Documents/GitHub/STA380_Take-Home-Exam-2/ReutersC50/*/*')
# Get author names from file paths
all_authors = path_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
lapply(function(x){substring(x,9)}) %>%
unlist
all_authors
library(modelr)
library(rsample)
library(foreach)
#file_list = Sys.glob('C:/Users/smjoh/Documents/GitHub/STA380/data/ReutersC50/*/*/*.txt')
file_list = Sys.glob('C:/Users/smjoh/Documents/GitHub/STA380_Take-Home-Exam-2/ReutersC50/*/*/*.txt')
#file_list = Sys.glob('../ReutersC50/*/*/*.txt')
all_authors_files = lapply(file_list, readerPlain)
#all_authors_files
# Clean up the file names
author_names = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '//') } %>%
unlist
# Rename the articles
names(all_authors_files) = author_names
## once you have documents in a vector, you
## create a text mining 'corpus' with:
documents_raw = Corpus(VectorSource(all_authors_files))
## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus
my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower))
# make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers))
# remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space
my_documents
## Remove stopwords.  Always be careful with this!
#stopwords("en")
#stopwords("SMART")
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
## create a doc-term-matrix
DTM_all_authors = DocumentTermMatrix(my_documents)
DTM_all_authors # some basic summary statistics
class(DTM_all_authors)
DTM_all_authors = removeSparseTerms(DTM_all_authors, 0.99)
DTM_all_authors
target = rep(all_authors, each=50)
bow_matrix = as.data.frame(as.matrix(DTM_all_authors))
bow_matrix['/TARGET/'] = target
target=append(target, target)
bow_matrix = subset(bow_matrix, select = -c(datetimestamp, isdst,listcontent, mday, mon, wday, yday) )
bow_matrix = as.data.frame(as.matrix(DTM_all_authors))
bow_matrix['/TARGET/'] = target
target = rep(all_authors, each=50)
target
target = rep(all_authors, each=50)
bow_matrix = as.data.frame(as.matrix(DTM_all_authors))
bow_matrix['/TARGET/'] = target
#target=append(target, target)
bow_matrix = subset(bow_matrix, select = -c(datetimestamp, isdst,listcontent, mday, mon, wday, yday) )
bow_matrix = as.data.frame(as.matrix(DTM_all_authors))
bow_matrix['/TARGET/'] = target
set.seed(123)
tr = sample(1:nrow(bow_matrix),4000)
training.set = bow_matrix[tr,]
test.set = bow_matrix[-tr,]
X_train = data.matrix(training.set[,-3379])
y_train = as.factor(training.set[,3379])
X_test = data.matrix(test.set[,-3379])
y_test = as.factor(test.set[,3379])
library(randomForest)
RFmodel <- randomForest(x = X_train,
y = y_train,
ntree = 100)
y_test_pred = predict(object = RFmodel, newdata = X_test)
tab = table(y_test, y_test_pred)
sum(diag(table(y_test, y_test_pred)))/length(y_test)
# Reading the billboard.csv file
billboard=read.csv("https://raw.githubusercontent.com/dpuelz/STA380/master/data/billboard.csv")
library(dplyr)
library(tidyverse)
library(kableExtra)
most_popular_songs=billboard %>%
group_by(performer,song) %>%
summarize(count=n(), .groups = 'drop') %>%
arrange(desc(count)) %>%
head(10)
knitr::kable(most_popular_songs, col.names = c("Performer","Song","Count"))
billboard %>%
filter(year!=1958 & year!=2021) %>% # Filtering out the years 1958 and 2021
group_by(year) %>%
summarize(musical_diversity=length(unique(song)))%>%
ggplot(aes(x = year, y = musical_diversity))+
geom_line()
labs(x = "Year", y = "Musical Diversity",caption="The musical diversity peaked in 1970 with about 800 unique songs that appeared in the Billboard Top 100. The musical diversity then decreased steadily, almost halfing in the beginning of the 21st century. The musical diversity has been continuously increasing since then reaching 1970 levels in 2020")
# Filtering out the 10-week hit songs
ten_week_hit = billboard %>%
group_by(performer, song) %>%
summarize(week_hits = n(), .groups='drop') %>%
filter(week_hits >= 10)
# Filtering out artists who had at least 30 songs that were 10 week hits
dat = artist_at_least_thirty_ten_week_hits=ten_week_hit %>%
group_by(performer) %>%
summarize(count_10_week_hits= n()) %>%
filter(count_10_week_hits >= 30)
#print(dim(artist_at_least_30_ten_week_hits))
plt1 = ggplot(dat, aes(x = performer, y = count_10_week_hits))+
geom_col()+
coord_flip()+
labs(x = "Performer", y = "No of 10 week hits")
#artist_at_least_thirty_ten_week_hits
plt1
library(dplyr)
library(mosaic)
library(tidyverse)
raw_buildings_data <-read.csv('https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv')
raw_buildings_data$renovated <- as.factor(raw_buildings_data$renovated )
raw_buildings_data$class_a <- as.factor(raw_buildings_data$class_a )
raw_buildings_data$class_b <- as.factor(raw_buildings_data$class_b )
raw_buildings_data$LEED <- as.factor(raw_buildings_data$LEED )
raw_buildings_data$Energystar <- as.factor(raw_buildings_data$Energystar )
raw_buildings_data$green_rating <- as.factor(raw_buildings_data$green_rating )
raw_buildings_data$amenities <- as.factor(raw_buildings_data$amenities )
raw_buildings_data$net <- as.factor(raw_buildings_data$net )
raw_buildings_data$class <- ifelse(raw_buildings_data$class_a == 1,'A',ifelse(raw_buildings_data$class_b == 1,'B','C'))
raw_buildings_data %>% filter(leasing_rate >10)%>%group_by(green_rating)%>%summarise(med_rent = median(Rent),  mean_rent = mean(Rent), count = n())
ggplot(raw_buildings_data, aes(green_rating, Rent)) + geom_boxplot()
ggplot(raw_buildings_data, aes(age,leasing_rate, color= green_rating)) + geom_point()
ggplot(raw_buildings_data, aes(leasing_rate, Rent, color = green_rating)) + geom_point()
raw_buildings_data %>% group_by(green_rating)%>%summarise(median_rate = median(leasing_rate), count = n())
raw_buildings_data%>%group_by(green_rating)%>%summarise(median_size = median(size), count = n())
ggplot(raw_buildings_data, aes(size,Rent,color = green_rating)) + geom_point()
raw_buildings_data%>%group_by(green_rating, amenities)%>%summarise(median(Rent),count=n())
raw_buildings_data%>%group_by(green_rating)%>%summarise(median(Rent),median(age),count=n())
ggplot(raw_buildings_data, aes(age,Rent,color = green_rating)) + geom_point()
raw_buildings_data%>%group_by(green_rating)%>%summarise(median(stories),median(Rent), count= n())
ggplot(raw_buildings_data, aes(stories,Rent, color = green_rating)) + geom_point()
raw_buildings_data%>%filter(cluster>=430 & cluster<=600)%>%group_by(green_rating)%>%summarise(median(Rent), count= n())
ggplot(raw_buildings_data, aes(cluster, Rent, color = green_rating)) + geom_point()
#Reading the CapMetro csv file
capmetro=read.csv("https://raw.githubusercontent.com/dpuelz/STA380/master/data/capmetro_UT.csv")
#print(capmetro)
# Extracting the data for average boarding happening every hour
avg_boarding_every_hour = capmetro %>%
group_by(hour_of_day, day_of_week) %>%
summarise(avgboarding = mean(boarding), .groups='drop') %>%
arrange(day_of_week)
#avg_boarding_every_hour
ggplot(avg_boarding_every_hour) +
geom_line(aes(y=avgboarding, x= hour_of_day))+
facet_wrap(~day_of_week)+
labs(title = "Capital Metro Boarding",y = "Average Boarding",x = "Time",caption="The average boarding on weekdays is much more than that on the weekends(Saturday/Sunday). The boarding steadily increases after 6:00 peaking at around 15:00-16:00 on all weekdays")
#Extracting the data for average alighting every hour
avg_alighting_every_hour = capmetro %>%
group_by(hour_of_day, day_of_week) %>%
summarise(avg_alighting = mean(alighting), .groups='drop') %>%
arrange(day_of_week)
#avg_alighting_every_hour
ggplot(avg_alighting_every_hour) +
geom_line(aes(y=avg_alighting , x= hour_of_day))+
facet_wrap(~day_of_week)+
labs(title = "Capital Metro Alighting",y = "Average Alighting",x = "Time",caption="The average alighting on weekdays is much more than that on the weekends(Saturday/Sunday). The alighting steadily increases after 6:00 peaking at around 8:00-9:00 on all weekdays, probably due to the fact that working people might be getting off the bus after taking a ride to office")
ggplot(data=capmetro,aes(x=temperature,y=boarding))+
geom_point() +
labs(title = "Boarding vs Temperature",y = "Boarding",x = "Temperature",caption="From the scatter plot above, the boarding looks independent of the temperature")
ggplot(data=capmetro,aes(x=temperature,y=alighting))+
geom_point() +
labs(title = "Alighting vs Temperature",y = "Alighting",x = "Temperature",caption="From the scatter plot above, the alighting also looks independent of the temperature")
library(mosaic)
library(quantmod)
library(foreach)
# Import sets of  ETFs
safe_pf = c("VOO", "SCHA", "VXUS", "VWO", "AOR")
high_pf = c("TQQQ","QLD","SPXL","SPUU","SWAN","NTSX")
med_pf = c("VOO","SCHA","VXUS","SPUU","SWAN","NTSX")
safe_pf.data = getSymbols(safe_pf, from = "2014-01-01")
high_pf.data = getSymbols(high_pf, from = "2014-01-01")
med_pf.data = getSymbols(med_pf, from = "2014-01-01")
#adjust for  splits and dividents
VOOa <- adjustOHLC(VOO)
SCHAa <- adjustOHLC(SCHA)
VXUSa <- adjustOHLC(VXUS)
VWOa <- adjustOHLC(VWO)
AORa <- adjustOHLC(AOR)
TQQQb <- adjustOHLC(TQQQ)
QLDb <- adjustOHLC(QLD)
SPXLb <- adjustOHLC(SPXL)
SPUUb <- adjustOHLC(SPUU)
SWANb <- adjustOHLC(SWAN)
NTSXb <- adjustOHLC(NTSX)
VOOc <- adjustOHLC(VOO)
SCHAc <- adjustOHLC(SCHA)
VXUSc <- adjustOHLC(VXUS)
SPUUc <- adjustOHLC(SPUU)
SWANc <- adjustOHLC(SWAN)
NTSXc <- adjustOHLC(NTSX)
all_returns_safe <- cbind(ClCl(VOOa),ClCl(SCHAa), ClCl(VXUSa),ClCl(VWOa),ClCl(AORa))
all_returns_high <- cbind(ClCl(TQQQb),ClCl(QLDb), ClCl(SPXLb),ClCl(SPUUb),ClCl(SWANb),ClCl(NTSXb))
all_returns_med <- cbind(ClCl(VOOc),ClCl(SCHAc), ClCl(VXUSc),ClCl(SPUUc),ClCl(SWANc),ClCl(NTSXc))
all_returns_safe = as.matrix(na.omit(all_returns_safe))
all_returns_high = as.matrix(na.omit(all_returns_high))
all_returns_med = as.matrix(na.omit(all_returns_med))
head(all_returns_safe)
head(all_returns_high)
head(all_returns_med)
pairs(all_returns_safe)
pairs(all_returns_high)
pairs(all_returns_med)
return.today_safe <- resample(all_returns_safe, 1, orig.ids = FALSE)
